---
title: 'CSC Evaluation: Survey response rates'
author: "Matt Mawer"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  word_document: default
  pdf_document: default
---

```{r setup, include = FALSE}

source("S:/SCHOLARSHIPS/CSC/SCHEMES/CSFP-IN/CSC-Evaluation/Data Management Crystal Snap IT/r_codebank/2016_sch_long/2016_sch_long_responserates.r")

library(pacman)
pacman:: p_load(tidyverse, pander,knitr, RODBC)

# Preferred option with this markdown is to knit to MS Word and apply style template saved for quick formatting.

```

This paper relates to the __`r SurveyName`__.  

Basic data on response rates is presented in the sections below, including comparisons within gender, scholar origin, PhD / non-PhD scholars, and year groups within the evaluation framework. Incidence of failed email contacts are highlighted separately in Section 2.

Response rates are calculated based on eligible recipients. For this dataset, there were `r sum(resp.overall$n)` eligible recipients. To be defined as eligible, a recipient must meet the criteria of the survey (e.g. our alumni surveys go to Scholars 2, 4, 6, 8, and 10 years after they complete CSC funding) and not have opted out of participating in evaluation research. Failed email addresses are _not_ excluded as eligible recipients and so the response rates reported here will typically be lower than those of the early evaluation program, when the rate was calculated based on those who _received_ the survey.

For more detail, please contact the evaluation team: [evaluation@cscuk.org.uk](mailto:evaluation@cscuk.org.uk)

# 1. Basic Response Rates {.tabset}

## Overall
  
```{r echo=FALSE, results='asis'}
rename(resp.overall, Cases = n,"Rate (%)" = Rate) %>% 
  pander()
```

## Year Group
  
Year Group refers to the evaluation group a Scholar is assigned to, based on when they completed their CSC funding. A Scholar who completed their funding in 2014 would be assigned to the 2014 year group.  

```{r echo=FALSE}
filter(resp.YearGroup, Response == "Complete") %>% 
  select(-Response, "Complete cases" = n,"Response rate (%)" = Rate) %>% 
  pander()

```

\pagebreak 

## Scheme

By nominating route:

```{r echo=FALSE}
filter(resp.sch, Response == "Complete") %>% 
  arrange(desc(Rate)) %>% 
  select("Nominating route"=SchemeNom,-Response, "Complete cases" = n,"Response rate (%)" = Rate) %>% 
  pander()
```

By scholarship type:

```{r echo=FALSE}
filter(resp.schtype, Response == "Complete") %>% 
  arrange(desc(Rate)) %>% 
  select("Scheme type"=SchemeType,-Response, "Complete cases" = n,"Response rate (%)" = Rate) %>% 
  pander()
```

## PhD

```{r echo=FALSE}
filter(resp.phd, Response == "Complete") %>% 
  arrange(desc(Rate)) %>% 
  select(Degree=PhD,-Response, "Complete cases" = n,"Response rate (%)" = Rate) %>% 
  pander()
```

## Scholar origin

Origin can be examined at country and regional level. It is usually more difficult to make informed analyses at country level because there is limited data for some countries (e.g. few surveys are sent to many of the smaller Commonwealth states). Regional analysis can help overcome this problem by aggregating data on countries, but it can also 'smooth over' variations between countries that may be of interest and thus should be treated with caution. In the first table, response rates are for countries for which at least 30 participants were included in the current survey.

```{r echo=FALSE}
filter(resp.country, sum(n)>30, Response=="Complete") %>% 
  arrange(desc(Rate)) %>% 
  select(-Response, "Complete cases" = n,"Response rate (%)" = Rate) %>% 
  pander()
```

In the second table, response rates are for aggregated regions (including all data).

```{r echo=FALSE}

filter(resp.region, Response == "Complete") %>% 
  arrange(desc(Rate)) %>% 
  select(-Response, "Complete cases" = n,"Response rate (%)" = Rate) %>% 
  pander()
```

# 2. Failed contacts {.tabset}

There were `r resp.overall %>% filter(Response=="Failed") %>% select(n)` failed email addresses (`r resp.overall %>% filter(Response=="Failed") %>% select(Rate)`%).

## Year Group

```{r echo=FALSE}
filter(resp.YearGroup, Response == "Failed") %>% 
  select(-Response, "Failed cases" = n,"Proportion (%)" = Rate) %>% 
  pander()
```

## Scheme

By nominating route:

```{r echo=FALSE}
filter(resp.sch, Response == "Failed") %>% 
  arrange(desc(Rate)) %>% 
  select("Nominating route"=SchemeNom,-Response, "Failed cases" = n,"Proportion (%)" = Rate) %>% 
  pander()
```

By scholarship type:

```{r echo=FALSE}
filter(resp.schtype, Response == "Failed") %>% 
  arrange(desc(Rate)) %>% 
  select("Scheme type"=SchemeType,-Response, "Failed cases" = n,"Proportion (%)" = Rate) %>% 
  pander()
```

## PhD

```{r echo=FALSE}
filter(resp.phd, Response == "Failed") %>% 
  arrange(desc(Rate)) %>% 
  select(Degree=PhD,-Response, "Failed cases" = n,"Proportion (%)" = Rate) %>% 
  pander()
```

## Scholar origin

In the first table, response rates are for countries for which at least 30 participants were included in the current survey.

```{r echo=FALSE}
filter(resp.country, sum(n)>30, Response=="Failed") %>% 
  arrange(desc(Rate)) %>% 
  select(-Response, "Failed cases" = n,"Proportion (%)" = Rate) %>% 
  pander()
```

In the second table, response rates are for aggregated regions (including all data).

```{r echo=FALSE}

filter(resp.region, Response == "Failed") %>% 
  arrange(desc(Rate)) %>% 
  select(-Response, "Failed cases" = n,"Proportion (%)" = Rate) %>% 
  pander()
```


# 3. Technical appendix {.tabset}

## Rationale

Examining the response rate helps to identify systemic non-response that might introduce bias into the data, such as a particular group that is regularly overly-represented in our analyses because they disproportionately complete the survey (or vice versa). Year-to-year random variations within the response rates of particular groups are to be expected in any research survey, but systemic non-response bias is a pressing concern for analytic validity and the CSC attempts to identify such problems where possible. Response rates are often viewed through the lens of the 'proportion' of the survey population covered, e.g. 10% of Scholars responded. While this is interesting information, it is not as important as systemic non-response bias for understanding the validity of analyses.

Numerical breakdown of response rates is indicative, but not explanatory. Separate analysis must be conducted to ascertain why eligible participants chose not to respond. Informally, the CSC is aware that internet or power outages and inaccessibility of internet whilst on field trips contribute to lower response rates for some eligible participants and that these pressures may disproportionately affect respondents who fall into certain groups (e.g. respondents residing in low income countries with unreliable infrastructure). To determine how far such factors shaped the cohort of survey respondents in any particular year is beyond the capacity of response rates alone to explain: richer contextual information is required.

## Data

Data used in this analysis come from the `r SurveyName`. The report is generated in RStudio using a response rates analysis script and a RMarkdown report: either file can viewed on request.

The structure of the dataset is:

```{r echo = FALSE} 
str(res.data)
```

The variables PhD and Response are both coded. PhD is binary, with 1 = PhD Scholarship recipient. In our coding of responses, 1 = Completed survey ("Complete"), 2 = Survey not completed, but no indication recipient did not receive the invitation ("Incomplete"), 3 = Failed email address ("Failed"), 4 = Planned to be included, but subsequently removed from survey (e.g. refused participation after invitation email).  Code 4 - Ineligible recipients - are excluded from this report as they are no longer part of the eligible population for the survey.