---
output:
  html_document: null
  pdf_document: default
  theme: yeti
  word_document: default
---

```{r setup, include = FALSE}
load("2016_sch_long_responserates.rdata")

library(pacman)
pacman:: p_load(openxlsx, dplyr, knitr, pander, plotly)

```


![](http://cscuk.dfid.gov.uk/wp-content/uploads/2014/12/csc-header-website.png)\

<br>**`r SurveyName`**: Response rates<br>
`r date()`

This paper relates to the `r SurveyName`. In the sections below we present data on the response rates, including comparisons within gender, scholar origin, PhD / non-PhD scholars, and year groups within the evaluation framework. Click on tabs below to move between different topics. 

For reference, 'n' is used to refer to the number of participants within a particular group (e.g. those who completed the survey; Jamaicans who completed the survey; PhD Scholars whose email failed, etc.) and 'Rate' describes the response rate for that group (e.g. x% of Jamaicans, out of all Jamaicans, completed the survey).

For more detailed interpretation (or explanation) of the figures, please contact the evaluation team: [evaluation@cscuk.org.uk](mailto:evaluation@cscuk.org.uk)

# 1. Basic Response Rates {.tabset}

Response rates are calculated based on eligible recipients. To be defined as eligible, a recipient must meet the criteria of the survey (e.g. our alumni surveys go to Scholars 2, 4, 6, 8, and 10 years after they complete CSC funding) and not have opted out of participating in evaluation research. Failed email addresses are _not_ excluded as eligible recipients and so the response rates reported here will typically be lower than those of the early evaluation program, when the rate was calculated based on those who _received_ the survey.

## Overall

```{r echo=FALSE, results='asis'}
pander(resp.overall)
```

## Year Group

Note: Year Group refers to the evaluation group a Scholar is assigned to, based on when they completed their CSC funding. A Scholar who completed their funding in 2014 would be assigned to the 2014 year group.

```{r echo=FALSE}
filter(resp.YearGroup, Response == "Completed") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

## Scheme

By nominating route:

```{r echo=FALSE}
filter(resp.sch, Response == "Completed") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

By scholarship type:

```{r echo=FALSE}
filter(resp.schtype, Response == "Completed") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

## PhD

```{r echo=FALSE}
filter(resp.phd, Response == "Completed") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

## Scholar origin

Origin can be examined at country and regional level. It is usually more difficult to make informed analyses at country level because there is limited data for some countries (e.g. few surveys are sent to many of the smaller Commonwealth states). Regional analysis can help overcome this problem by aggregating data on countries, but it can also 'smooth over' variations between countries that may be of interest.

In the first table are response rates for countries for which at least 30 participants were included in the current survey.

```{r echo=FALSE}
filter(resp.country, sum(n)>30, Response=="Completed") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

In the second table are response rates for aggregated regions (including all data).

```{r echo=FALSE}

filter(resp.region, Response == "Completed") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

# 2. Failed contacts {.tabset}

The CSC evaluation, programme, and alumni teams help to keep records updated and provide usable contact details for Scholars. However, sometimes these details fail and we are unable to send the survey. Failed email addresses are not removed from response rate calculations: these individuals should have the opportunity to participate in the survey and so are included here as a special category of 'non-completions'.

As noted above, there were `r resp.overall %>% filter(Response==Failed_contact) %>% select(n)` failed email addresses (`r resp.overall %>% filter(Response ==Failed_contact) %>% select(Rate)`%). The tabs below give a breakdown of failed emails by various categories.

## Year Group

Note: Year Group refers to the evaluation group a Scholar is assigned to, based on when they completed their CSC funding. A Scholar who completed their funding in 2014 would be assigned to the 2014 year group.

```{r echo=FALSE}
filter(resp.YearGroup, Response == "Failed_contact") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

## Scheme

By nominating route:

```{r echo=FALSE}
filter(resp.sch, Response == "Failed_contact") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

By scholarship type:

```{r echo=FALSE}
filter(resp.schtype, Response == "Failed_contact") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

## PhD

```{r echo=FALSE}
filter(resp.phd, Response == "Failed_contact") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

## Scholar origin

Origin can be examined at country and regional level. It is usually more difficult to make informed analyses at country level because there is limited data for some countries (e.g. few surveys are sent to many of the smaller Commonwealth states). Regional analysis can help overcome this problem by aggregating data on countries, but it can also 'smooth over' variations between countries that may be of interest.

In the first table are response rates for countries for which at least 30 participants were included in the current survey.

```{r echo=FALSE}
filter(resp.country, sum(n)>30, Response=="Failed_contact") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```

In the second table are response rates for aggregated regions (including all data).

```{r echo=FALSE}

filter(resp.region, Response == "Failed_contact") %>% 
  arrange(desc(Rate)) %>% 
  pander()
```


# 3. Technical appendix {.tabset}

## Rationale

Response rates are often viewed through the lens of the 'proportion' of the survey population covered, e.g. 10% of Scholars responded. While this is interesting information, it is not the most important factor for ensuring our analyses of the data are meaningful: it has little mathematical impact on analyses, for instance, as long as the sample size is large enough. We routinely examine the response rate to help identify systemic non-response that might introduce bias into the data, such as a particular group that is regularly overly-represented in our analyses because they disproportionately complete the survey, or vice versa. 

Providing a breakdown of response rates to specific surveys is a necessary facet of examining the reliability of our research, but it is not sufficient alone. It cannot tell us, for instance, why eligible participants chose not to respond. We know informally that internet or power outages and inaccessibility of internet or emails whilst on field trips contribute to lower response rates for some eligible participants and that these pressures may disproportinately affect respondents who fall into certain groups (e.g. respondents residing in low income countries with unreliable infrastructure). 

Nonetheless, if our response analysis can show that either there is no evidence of systemic bias in the survey results, or can identify such bias and allow us to take it into account in subsequent work, then we can be confident that our substantive analysis of the survey findings will be robust.

## Data

Data used in this analysis come from the `r SurveyName`. The structure of the dataset is:

```{r echo = FALSE} 
str(res.data)
```

The variables PhD and Response are both coded. 

For PhD, 1 = PhD Scholarship recipient; 0 = Other Scholarship recipient. 

For response, 1 = Completed survey, 2 = Received, but not completed, survey, 3 = Failed email address, 4 = Planned to be included, but subsequently removed from survey (e.g. refused participation after invitation email). Simple.response turns the 'Response' variable into a binary (completed or non-response) which is easier to understand, but it should be noted that Simple.response will count those not included in the survey (Response = 4) as non-responses, which may be misleading.